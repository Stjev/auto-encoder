{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac295ec4-a6e9-42c5-8d99-c12189fff15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d73ca74-0b08-40ee-9c9d-3232164b520e",
   "metadata": {},
   "source": [
    "# Create a Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff2ef1-de91-482b-93ba-c414cab8be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_W, IMAGE_H = 512, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99b107-69b8-431e-80d4-ab06699927ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, train_val_test=0):\n",
    "        self.images = glob(f\"./new_ds/{['train', 'val', 'test'][train_val_test]}/*.png\")\n",
    "        self.transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Normalize(mean=[0.28689554, 0.32513303, 0.28389177], \n",
    "                                             std=[0.18696375, 0.19017339, 0.18720214])\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)       \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        # load and transform image\n",
    "        img_pil = self.transform(torchvision.io.read_image(img_path) / 255.)\n",
    "        return img_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b053e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ds = ImageDataset()\n",
    "test_img = img_ds[4]\n",
    "\n",
    "print(test_img.mean())\n",
    "print(test_img.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8317987-20b2-482f-96ad-8b137b80ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(img_ds))\n",
    "\n",
    "# t1 = time.time()\n",
    "\n",
    "# for img in img_ds:\n",
    "#     y = img\n",
    "    \n",
    "# print(f\"Total time to go through ds: {time.time() - t1:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422244ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(x):\n",
    "    device = 'cuda' if x.is_cuda else 'cpu'\n",
    "    \n",
    "    x = x.permute(2, 1, 0)\n",
    "    x *= torch.tensor([0.18696375, 0.19017339, 0.18720214]).to(device)\n",
    "    x += torch.tensor([0.28689554, 0.32513303, 0.28389177]).to(device)\n",
    "    return x.permute(2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5696c0-b010-4a26-ba11-22e7e5880df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = visualize(img_ds[4]).unsqueeze(0)\n",
    "\n",
    "print(\"min:\", test_img.min())\n",
    "print(\"max:\", test_img.max())\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(test_img.squeeze(0).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92448c32-006e-404b-9d49-4ecf9753c406",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c92acc01-5813-477d-97a0-8389513d7a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\ttorch.Size([1, 3, 256, 512])\n",
      "Model out\ttorch.Size([1, 3, 256, 512])\n"
     ]
    }
   ],
   "source": [
    "# CURRENT: https://medium.com/@tioluwaniaremu/vgg-16-a-simple-implementation-using-pytorch-7850be4d14a1\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # 3, 256, 512\n",
    "        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        # 64, 128, 256\n",
    "        self.conv2_1 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        # 128, 64, 128\n",
    "        self.conv3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        # 256, 32, 64\n",
    "        self.conv4_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        # 512, 16, 32\n",
    "        self.conv5_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        # 512, 8, 16\n",
    "        self.conv6_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv6_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv6_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        # 512, 4, 8        \n",
    "        self.convup6_3 = nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)\n",
    "        self.convup6_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.convup6_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        # 512, 8, 16\n",
    "        self.convup5_3 = nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)\n",
    "        self.convup5_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.convup5_1 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1)\n",
    "        # 512, 16, 32\n",
    "        self.convup4_3 = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=2, stride=2)\n",
    "        self.convup4_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.convup4_1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
    "        # 256, 32, 64\n",
    "        self.convup3_3 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=2, stride=2)\n",
    "        self.convup3_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.convup3_1 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n",
    "        # 128, 64, 128\n",
    "        self.convup2_3 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=2, stride=2)\n",
    "        self.convup2_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.convup2_1 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        # 64, 128, 256\n",
    "        self.convup1_3 = nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=2, stride=2)\n",
    "        self.convup1_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.convup1_1 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        # 64, 256, 512\n",
    "        self.convup0_1 = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=1)\n",
    "        # 3, 256, 512\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = F.relu(self.conv3_3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv4_1(x))\n",
    "        x = F.relu(self.conv4_2(x))\n",
    "        x = F.relu(self.conv4_3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv5_1(x))\n",
    "        x = F.relu(self.conv5_2(x))\n",
    "        x = F.relu(self.conv5_3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv6_1(x))\n",
    "        x = F.relu(self.conv6_2(x))\n",
    "        x = F.relu(self.conv6_3(x))\n",
    "        x = self.maxpool(x)\n",
    "        # decoder\n",
    "        x = F.relu(self.convup6_3(x))\n",
    "        x = F.relu(self.convup6_2(x))\n",
    "        x = F.relu(self.convup6_1(x))\n",
    "        x = F.relu(self.convup5_3(x))\n",
    "        x = F.relu(self.convup5_2(x))\n",
    "        x = F.relu(self.convup5_1(x))\n",
    "        x = F.relu(self.convup4_3(x))\n",
    "        x = F.relu(self.convup4_2(x))\n",
    "        x = F.relu(self.convup4_1(x))\n",
    "        x = F.relu(self.convup3_3(x))\n",
    "        x = F.relu(self.convup3_2(x))\n",
    "        x = F.relu(self.convup3_1(x))\n",
    "        x = F.relu(self.convup2_3(x))\n",
    "        x = F.relu(self.convup2_2(x))\n",
    "        x = F.relu(self.convup2_1(x))\n",
    "        x = F.relu(self.convup1_3(x))\n",
    "        x = F.relu(self.convup1_2(x))\n",
    "        x = F.relu(self.convup1_1(x))\n",
    "        x = F.relu(self.convup0_1(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(f\"Original\\t{test_img.shape}\")\n",
    "out = Model()(test_img)\n",
    "print(f\"Model out\\t{out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98c3cafe-bc35-429f-b54a-6752805e0653",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 512]           1,792\n",
      "            Conv2d-2         [-1, 64, 256, 512]          36,928\n",
      "         MaxPool2d-3         [-1, 64, 128, 256]               0\n",
      "            Conv2d-4        [-1, 128, 128, 256]          73,856\n",
      "            Conv2d-5        [-1, 128, 128, 256]         147,584\n",
      "         MaxPool2d-6         [-1, 128, 64, 128]               0\n",
      "            Conv2d-7         [-1, 256, 64, 128]         295,168\n",
      "            Conv2d-8         [-1, 256, 64, 128]         590,080\n",
      "            Conv2d-9         [-1, 256, 64, 128]         590,080\n",
      "        MaxPool2d-10          [-1, 256, 32, 64]               0\n",
      "           Conv2d-11          [-1, 512, 32, 64]       1,180,160\n",
      "           Conv2d-12          [-1, 512, 32, 64]       2,359,808\n",
      "           Conv2d-13          [-1, 512, 32, 64]       2,359,808\n",
      "        MaxPool2d-14          [-1, 512, 16, 32]               0\n",
      "           Conv2d-15          [-1, 512, 16, 32]       2,359,808\n",
      "           Conv2d-16          [-1, 512, 16, 32]       2,359,808\n",
      "           Conv2d-17          [-1, 512, 16, 32]       2,359,808\n",
      "        MaxPool2d-18           [-1, 512, 8, 16]               0\n",
      "           Conv2d-19           [-1, 512, 8, 16]       2,359,808\n",
      "           Conv2d-20           [-1, 512, 8, 16]       2,359,808\n",
      "           Conv2d-21           [-1, 512, 8, 16]       2,359,808\n",
      "        MaxPool2d-22            [-1, 512, 4, 8]               0\n",
      "  ConvTranspose2d-23           [-1, 512, 8, 16]       1,049,088\n",
      "           Conv2d-24           [-1, 512, 8, 16]       2,359,808\n",
      "           Conv2d-25           [-1, 512, 8, 16]       2,359,808\n",
      "  ConvTranspose2d-26          [-1, 512, 16, 32]       1,049,088\n",
      "           Conv2d-27          [-1, 512, 16, 32]       2,359,808\n",
      "           Conv2d-28          [-1, 512, 16, 32]       2,359,808\n",
      "  ConvTranspose2d-29          [-1, 512, 32, 64]       1,049,088\n",
      "           Conv2d-30          [-1, 512, 32, 64]       2,359,808\n",
      "           Conv2d-31          [-1, 256, 32, 64]       1,179,904\n",
      "  ConvTranspose2d-32         [-1, 256, 64, 128]         262,400\n",
      "           Conv2d-33         [-1, 256, 64, 128]         590,080\n",
      "           Conv2d-34         [-1, 128, 64, 128]         295,040\n",
      "  ConvTranspose2d-35        [-1, 128, 128, 256]          65,664\n",
      "           Conv2d-36        [-1, 128, 128, 256]         147,584\n",
      "           Conv2d-37         [-1, 64, 128, 256]          73,792\n",
      "  ConvTranspose2d-38         [-1, 64, 256, 512]          16,448\n",
      "           Conv2d-39         [-1, 64, 256, 512]          36,928\n",
      "           Conv2d-40         [-1, 64, 256, 512]          36,928\n",
      "           Conv2d-41          [-1, 3, 256, 512]             195\n",
      "================================================================\n",
      "Total params: 39,445,379\n",
      "Trainable params: 39,445,379\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.50\n",
      "Forward/backward pass size (MB): 644.62\n",
      "Params size (MB): 150.47\n",
      "Estimated Total Size (MB): 796.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(Model().cuda(), input_size=(3, 256, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d82b2-117a-4aa5-b565-bb135e5efb90",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d266719e-0137-4209-9e20-46d56fee1c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1000\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1fe6afe-04a0-4bc6-9767-88fc556b9cb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 11.00 GiB total capacity; 10.15 GiB already allocated; 0 bytes free; 10.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, batch)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvup3_2(x))\n\u001b[1;32m     94\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvup3_1(x))\n\u001b[0;32m---> 95\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvup2_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvup2_2(x))\n\u001b[1;32m     97\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvup2_1(x))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 11.00 GiB total capacity; 10.15 GiB already allocated; 0 bytes free; 10.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('runs/vgg16' + '_' + datetime.datetime.now().strftime(\"%d%m-%H%M%S\"))\n",
    "\n",
    "# intialize model\n",
    "m = Model().cuda()\n",
    "\n",
    "# initialize optimizer \n",
    "optim = torch.optim.Adam(m.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# dataloader \n",
    "dl_train = torch.utils.data.DataLoader(ImageDataset(train_val_test=0), batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "# set up validation datasets and validation images\n",
    "val_ds = ImageDataset(train_val_test=1)\n",
    "dl_val = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "val_img1, val_img2, val_img3 = val_ds[14], val_ds[307], val_ds[450]\n",
    "\n",
    "grid = torch.zeros((3, 256 * 2, 512 * 3))\n",
    "grid[:, :256, 0:512] = visualize(val_img1)\n",
    "grid[:, :256, 512:512*2] = visualize(val_img2)\n",
    "grid[:, :256, 512*2:512*3] = visualize(val_img3)\n",
    "\n",
    "val_set = torch.stack((val_img1, val_img2, val_img3))\n",
    "\n",
    "# loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "best_loss, patience = 10000, 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_sum = 0\n",
    "    t1 = time.time()\n",
    "    for i, batch in enumerate(dl_train):\n",
    "        # zero the gradient\n",
    "        optim.zero_grad()\n",
    "        # put the batch on GPU\n",
    "        batch = batch.cuda()\n",
    "        # forward pass\n",
    "        out = m(batch)\n",
    "        # calculate loss\n",
    "        loss = criterion(out, batch)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optim.step()\n",
    "        # add loss to loss sum\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # validation and metric logging\n",
    "        train_loss = loss_sum / len(dl_train)\n",
    "        val_loss = 0\n",
    "        \n",
    "        # calculate validation loss\n",
    "        for batch in dl_val:\n",
    "            batch = batch.cuda()\n",
    "            # forward pass\n",
    "            out = m(batch)\n",
    "            # calculate loss\n",
    "            loss = criterion(out, batch)\n",
    "            # add loss to loss sum\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "        # average the validation loss\n",
    "        val_loss /= len(dl_val)\n",
    "        # create validation images\n",
    "        val_out = m(val_set.cuda())\n",
    "    \n",
    "        grid[:, 256:, 0:512] = torch.clamp(visualize(val_out[0]), 0, 1)\n",
    "        grid[:, 256:, 512:512*2] = torch.clamp(visualize(val_out[1]), 0, 1)\n",
    "        grid[:, 256:, 512*2:512*3] = torch.clamp(visualize(val_out[2]), 0, 1)\n",
    "\n",
    "        writer.add_image('images', grid, epoch)\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Val/train', val_loss, epoch)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            \n",
    "        if patience == 10:\n",
    "            print(\"No new best model achieved, stopping here.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"[{epoch + 1}/{EPOCHS}]\\t({time.time() - t1:.2f}s)\\tloss: {train_loss:.4f}\\tval_loss: {val_loss:.4f}\\tpatience: {patience}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083e31d-fc6e-470c-96e1-554ebff6fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.clamp(visualize(m(img_ds[0].unsqueeze(0).cuda()).squeeze(0).detach().cpu()).permute(1, 2, 0), 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc827c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
